{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport pydicom\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf\nimport random\nimport sklearn\nimport scipy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load libraries\nfrom pandas import read_csv\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n# Algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/osic-pulmonary-fibrosis-progression/train.csv\")\ntrain_df.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\n\ntest_df = pd.read_csv(\"../input/osic-pulmonary-fibrosis-progression/test.csv\")\n\nsub_df = pd.read_csv(\"../input/osic-pulmonary-fibrosis-progression/sample_submission.csv\")\nsub_df['Patient'] = sub_df['Patient_Week'].apply(lambda x:x.split('_')[0])\nsub_df['Weeks'] = sub_df['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n#sub_df =  sub_df[['Patient','Weeks','Confidence','Patient_Week']]\nsub_test_df = sub_df.merge(test_df.drop('Weeks', axis=1), on=\"Patient\")\n\n#print(train_df.head())\nprint('Train shape: ', train_df.shape)\nprint('Number of unique patients : {}'.format(train_df['Patient'].nunique()))\n\nprint(test_df.head())\nprint('Test shape:', test_df.shape)\n#print('Number of patiens in test: {}'.format(test_df['Patient'].nunique()))\n\nprint('Submission shape: ',sub_df.shape)\nprint('Number of patients in submission : {}'.format(sub_df['Patient'].nunique()))\n#print(sub_test_df.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['FVC'].hist()\n#this is a skewed histograph, let's see if each patient's change in FVC is also a skewd distribution","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df['ChangeFVC'].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis temporal trends of FVC development","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot time v.s. FVC trend for each patient\n\ntrain_df[\"Patient\"].nunique()\nPlist = list(train_df.Patient.unique())\nPID='ID00007637202177411956430'\n#train_df.loc[train_df.Patient == PID]\n\npat=train_df.groupby(['Patient'])['FVC'].count().to_frame().reset_index()\nprint(pat.head())\n\nWeek = np.arange(-12, 134)\nPlist = list(train_df.Patient.unique())\ntrain_df2 = pd.DataFrame(Week, columns = [\"Weeks\"])\ntrain_df2.insert(1, 'FVC', np.nan)\ntrain_df2.insert(2, 'Percent', np.nan)\ntrain_df2.insert(3, 'Age', np.nan)\ntrain_df2.insert(4, 'Sex', np.nan)\ntrain_df2.insert(5, 'SmokingStatus', np.nan)\n\n\n# highlight a specific patient\nPID = train_df.loc[train_df.Patient == Plist[3]]\nPID = PID.reset_index()\nprint(train_df2.head())\n\nfor i, D in enumerate(PID.Weeks):\n    D = D + 12\n    train_df2.at[D, \"FVC\"] = PID.FVC[i]\n    train_df2.at[D, \"Percent\"] = PID.Percent[i]\n\ntrain_df2.loc[:, \"Age\"] = PID.Age[0]\ntrain_df2.loc[:, \"Sex\"] = PID.Sex[0]\ntrain_df2.loc[:, \"SmokingStatus\"] = PID.SmokingStatus[0]\n    \ntrain_df2 = train_df2.interpolate('linear', order=2, limit_direction='both')\ntrain_df2\n#view the FVC v.s. Weeks for one specific patient\nplt.figure(figsize=(18,6))\ngrp = train_df\n\nplt.xlabel(\"Weeks\")\nplt.ylabel(\"FVC\")\nplt.plot(grp.Weeks, grp.FVC, marker=\"x\")\nplt.plot(PID.Weeks, PID.FVC, marker=\"o\", markersize=8)\n\nPlist=pd.DataFrame(Plist)\nprint(Plist)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Time progression\nWeeks v.s. delta FVC%","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Train dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# set a base level (1'st week) for all data\ntrain_base = train_df.drop_duplicates(subset='Patient', keep='first')\n\ntrain_base = train_base[['Patient', 'Weeks', 'FVC', \n                   'Percent', 'Age']].rename(columns={'Weeks': 'BaseWeek',\n                                                      'Percent': 'BasePercent',\n                                                      'Age': 'BaseAge',\n                                                      'FVC': 'BaseFVC'})\n\ntrain_df = pd.merge(train_df,train_base,on='Patient',how='left')\nprint(train_df.head())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set a last data (the train_last is not used in fitting, this is just to view the change of FVC in the final week)\n#view a histgram of percentage change\ntrain_last = train_df.drop_duplicates(subset='Patient', keep='last')\ntrain_last = train_last[['Patient', 'Weeks', 'FVC', \n                   'Percent', 'Age']].rename(columns={'Weeks': 'LastWeek',\n                                                      'Percent': 'LastPercent',\n                                                      'Age': 'LastAge',\n                                                      'FVC': 'LastFVC'})\ntrain_last = pd.merge(train_df,train_last,on='Patient',how='left')\n\n\ntrain_last['lastChangeFVC'] = (train_last['LastFVC']-train_last['BaseFVC'])/train_last['BaseFVC']\ntrain_last['TimePassed'] = (train_last['LastWeek']-train_last['Weeks'])\ntrain_last = train_last.loc[train_last['TimePassed']==0,:]\nprint(train_last.head())\ntrain_last[\"lastChangeFVC\"].hist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"This is also a skewd distribution, so I'm going to resolve this into two seperate normal distribution to make it two different patient classes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pylab import *\nfrom scipy.optimize import curve_fit\n\ndata=train_last[\"lastChangeFVC\"]\ny,x,_=hist(data,label='data')\n\nx=(x[1:]+x[:-1])/2 # for len(x)==len(y)\n\ndef gauss(x,mu,sigma,A):\n    return A*exp(-(x-mu)**2/2/sigma**2)\n\ndef bimodal(x,mu1,sigma1,A1,mu2,sigma2,A2):\n    return gauss(x,mu1,sigma1,A1)+gauss(x,mu2,sigma2,A2)\n\nexpected=(0,0.1,4,-0.1,0.1,4)\nparams,cov=curve_fit(bimodal,x,y,expected)\nsigma=sqrt(diag(cov))\nplot(x,bimodal(x,*params),color='red',lw=3,label='model')\nlegend()\nprint(params,'\\n',sigma)  \nprint(params)  \n# via pandas :\n# pd.DataFrame(data={'params':params,'sigma':sigma},index=bimodal.__code__.co_varnames[1:])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#later split the dataset into two classes, set the threshold at 0.03\n\ntrain_last[\"class\"]=0\n\n##class1 <= -0.03\n#id1=train_last.index[train_last[\"lastChangeFVC\"]<=-0.03]\n#id2=train_last.index[train_last[\"lastChangeFVC\"]>=-0.03]\n\n#class1 <= 0\nid1=train_last.index[train_last[\"lastChangeFVC\"]<=0]\nid2=train_last.index[train_last[\"lastChangeFVC\"]>=0]\ntrain_last.at[id2,\"class\"]=1\n\n\n\nClass1ID=train_last.loc[id1,\"Patient\"]\nClass2ID=train_last.loc[id2,\"Patient\"]\nprint(shape(Class1ID))\nshape(train_last)\ntrain_last=train_last.iloc[:,np.r_[0,15,17]]\nprint(train_last.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#later split the dataset into two classes, set the threshold at 0.03","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate change for FVC and time\n\ntrain_df = train_df.merge(train_last, on=\"Patient\")\nprint(train_df.head())\ntrain_df['Visit'] = 1\ntrain_df['Visit'] = train_df[['Patient', 'Visit']].groupby('Patient').cumsum()\ntrain_df['ChangeFVC'] = (train_df['FVC']-train_df['BaseFVC'])/train_df['BaseFVC']\ntrain_df['TimePassed'] = (train_df['Weeks']-train_df['BaseWeek'])\nprint(train_df.head(150))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove base line data\n\ntrain_df = train_df.loc[train_df['TimePassed']>0,:]\n\ntrain_df = pd.get_dummies(train_df, columns=['Sex'])\ntrain_df = pd.get_dummies(train_df, columns=['SmokingStatus'])\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#later split the dataset into two classes, set the threshold at 0.03\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get rid of redundant data\ntrain_input=train_df.iloc[:,np.r_[0,1,2:4,6,7,10,12:19]]\ntrain_input.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate a mask for the upper triangle (taken from seaborn example gallery)\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\ncorr=train_input.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.subplots(figsize = (12,8))\nsns.heatmap(corr, \n            annot=True,\n            mask = mask,\n            cmap = 'RdBu',\n            linewidths=.9, \n            linecolor='white',\n            vmax = 0.3,\n            fmt='.2f',\n            center = 0,\n            square=True)\nplt.title(\"Correlations Matrix\", y = 1,fontsize = 20, pad = 20);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get a correlation matrix with Change in FVC (%)\npd.DataFrame(abs(train_input.corr()['ChangeFVC']).sort_values(ascending = False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##get a correlation matrix with FVC\npd.DataFrame(abs(train_input.corr()['FVC']).sort_values(ascending = False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_input.to_csv(\"class.csv\")\nClass1ID.to_csv('PatientID.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictive Modeling with sklearn Models (LR, LDA, KNN, CART, NB, SVM)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#FVC\tPercent\tBaseFVC\tBasePercent\tChangeFVC\tTimePassed\tSex_Female\tSex_Male\tSmokingStatus_Currently smokes\tSmokingStatus_Ex-smoker\tSmokingStatus_Never smoked\nX = train_input.iloc[:,np.r_[1,2,3,4,6:11]]\n\n\n#change float to interger as the models does not work on floats\n#y = train_input.iloc[:,1]  #FVC\ny = train_input.iloc[:,4]*1000 #ChangeFVC\ny = train_input.iloc[:,1]*10 #Percent\ny =  [round(x) for x in y]\ny = pd.DataFrame(y,columns=['ChangeFVC'])\n#y = train_input.iloc[:,1]\n\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.10, random_state=1)  #test accuracy using 10% of the sample size\nX_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.10, random_state=0)  #test accuracy using 10% of the sample size\n\n\nprint(X.head())\nprint(y.head())\nX_train.shape,X_test.shape,Y_train.shape,Y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Spot Check Algorithms\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n\tkfold = StratifiedKFold(n_splits=3, random_state=1, shuffle=True)\n\tcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n\tresults.append(cv_results)\n\tnames.append(name)\n\tprint('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compare Algorithms\npyplot.boxplot(results, labels=names)\npyplot.title('Algorithm Comparison')\npyplot.show()\n1\n2\n3\n4\n# Compare Algorithms\npyplot.boxplot(results, labels=names)\npyplot.title('Algorithm Comparison')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Make predictions on validation dataset\nmodel = SVC(gamma='auto')\nmodel.fit(X_train, Y_train)\npredictions = model.predict(X_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate predictions\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))\n#  TAT","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}